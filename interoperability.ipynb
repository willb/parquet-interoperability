{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interested-petersburg",
   "metadata": {},
   "source": [
    "# Parquet interoperability from the JVM to Python\n",
    "\n",
    "Apache Parquet is a great default choice for a data serialization format in data processing and machine learning pipelines, but just because it's available in many environments doesn't mean it has the same behavior everywhere. In the remaining discussion, we'll look at some potential interoperability headaches and show how to work around them.\n",
    "\n",
    "We'll start by looking at a Parquet file generated by Apache Spark with the output of an ETL job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loose-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-egyptian",
   "metadata": {},
   "source": [
    "We can look at the schema for this file and inspect a few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "second-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = session.read.parquet(\"colors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minute-announcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>YesNo</th>\n",
       "      <th>Color</th>\n",
       "      <th>Categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000267</td>\n",
       "      <td>No</td>\n",
       "      <td>red</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000004c2</td>\n",
       "      <td>No</td>\n",
       "      <td>red</td>\n",
       "      <td>ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00002dcf</td>\n",
       "      <td>No</td>\n",
       "      <td>blue</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000035be</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>2f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005f19</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>0a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00007c1e</td>\n",
       "      <td>No</td>\n",
       "      <td>blue</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000be2c</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000d29d</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000d313</td>\n",
       "      <td>Yes</td>\n",
       "      <td>blue</td>\n",
       "      <td>f7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000d66c</td>\n",
       "      <td>No</td>\n",
       "      <td>blue</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rowID YesNo  Color Categorical\n",
       "0  00000267    No    red          62\n",
       "1  000004c2    No    red          ba\n",
       "2  00002dcf    No   blue          75\n",
       "3  000035be    No  green          2f\n",
       "4  00005f19    No  green          0a\n",
       "5  00007c1e    No   blue          79\n",
       "6  0000be2c    No  green          38\n",
       "7  0000d29d    No  green          60\n",
       "8  0000d313   Yes   blue          f7\n",
       "9  0000d66c    No   blue          94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-durham",
   "metadata": {},
   "source": [
    "The \"file\" we're reading from (`colors.parquet`) is a partitioned Parquet file, so it's really a directory.  We can inspect the Parquet metadata for each column using the `parquet-tools` utility; this shows us that many of our columns are strings and that string columns are universally dictionary-encoded, which means that each string is stored as an index into a dictionary rather than as a literal value.  By storing values that may be repeated many times in this way, we save space and compute time.  (Parquet defaults to dictionary-encoding small-cardinality string columns, and we can assume that many of these will be treated as categoricals later in a data pipeline.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assigned-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rowID:        BINARY SNAPPY DO:0 FPO:4 SZ:4931389/8438901/1.71 VC:703200 ENC:RLE,BIT_PACKED,PLAIN ST:[min: 00000267, max: ffffc225, num_nulls: 0]\r\n",
      "YesNo:        BINARY SNAPPY DO:0 FPO:4931393 SZ:105082/108599/1.03 VC:703200 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: No, max: Yes, num_nulls: 0]\r\n",
      "Color:        BINARY SNAPPY DO:0 FPO:5036475 SZ:177524/177487/1.00 VC:703200 ENC:BIT_PACKED,PLAIN_DICTIONARY ST:[min: blue, max: red, num_nulls: 0]\r\n",
      "Categorical:  BINARY SNAPPY DO:0 FPO:5213999 SZ:705931/706389/1.00 VC:703200 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 00, max: ff, num_nulls: 0]\r\n"
     ]
    }
   ],
   "source": [
    "!parquet-tools meta colors.parquet 2>& 1 | head -70 | grep SNAPPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-candle",
   "metadata": {},
   "source": [
    "So far, so good.  But what happens when we read these data into pandas?  We can load Parquet files into pandas if we have PyArrow installed; let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "third-marble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>YesNo</th>\n",
       "      <th>Color</th>\n",
       "      <th>Categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000267</td>\n",
       "      <td>No</td>\n",
       "      <td>red</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000004c2</td>\n",
       "      <td>No</td>\n",
       "      <td>red</td>\n",
       "      <td>ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00002dcf</td>\n",
       "      <td>No</td>\n",
       "      <td>blue</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000035be</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>2f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005f19</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>0a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703195</th>\n",
       "      <td>ffff69a9</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703196</th>\n",
       "      <td>ffff8037</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703197</th>\n",
       "      <td>ffffa49f</td>\n",
       "      <td>No</td>\n",
       "      <td>red</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703198</th>\n",
       "      <td>ffffa6ae</td>\n",
       "      <td>No</td>\n",
       "      <td>green</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703199</th>\n",
       "      <td>ffffc225</td>\n",
       "      <td>Yes</td>\n",
       "      <td>blue</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>703200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rowID YesNo  Color Categorical\n",
       "0       00000267    No    red          62\n",
       "1       000004c2    No    red          ba\n",
       "2       00002dcf    No   blue          75\n",
       "3       000035be    No  green          2f\n",
       "4       00005f19    No  green          0a\n",
       "...          ...   ...    ...         ...\n",
       "703195  ffff69a9    No  green          25\n",
       "703196  ffff8037    No  green          34\n",
       "703197  ffffa49f    No    red          3a\n",
       "703198  ffffa6ae    No  green          89\n",
       "703199  ffffc225   Yes   blue          40\n",
       "\n",
       "[703200 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_parquet(\"colors.parquet/\")\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-infection",
   "metadata": {},
   "source": [
    "The data look about like we'd expect them to.  However, when we look at how pandas is representing our data, we're in for a nasty surprise:  pandas has taken our efficiently dictionary-encoded strings and represented them with arbitrary Python objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-irish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowID          object\n",
       "YesNo          object\n",
       "Color          object\n",
       "Categorical    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-weather",
   "metadata": {},
   "source": [
    "We could convert each column to strings and then to categoricals, but this would be tedious and it is also certainly wasteful in terms of time and space.  (Note that if we'd created a pandas table with `string` or `categorical` `dtypes` and saved _that_ to Parquet, the types would survive a round-trip to disk because they'd be stored in pandas-specific Parquet metadata.)\n",
    "\n",
    "In this case, pandas is using the PyArrow Parquet backend; interestingly, if we use PyArrow directly and read into a `pyarrow.Table`, the string types are preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eligible-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "arrow_table = pq.read_table(\"colors.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-glance",
   "metadata": {},
   "source": [
    "...but once we convert _that_ table to pandas, we've lost the type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "induced-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowID          object\n",
       "YesNo          object\n",
       "Color          object\n",
       "Categorical    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_table.to_pandas().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-consultancy",
   "metadata": {},
   "source": [
    "However, we can force PyArrow to preserve the dictionary encoding even through the pandas conversion if we specify the `read_dictionary` option with a list of appropriate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honey-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_arrow_table = pq.read_table(\"colors.parquet/\", read_dictionary=['YesNo', 'Color', 'Categorical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "downtown-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "rowID: string\n",
       "YesNo: dictionary<values=string, indices=int32, ordered=0>\n",
       "Color: dictionary<values=string, indices=int32, ordered=0> not null\n",
       "Categorical: dictionary<values=string, indices=int32, ordered=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_arrow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "muslim-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowID            object\n",
       "YesNo          category\n",
       "Color          category\n",
       "Categorical    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_arrow_table.to_pandas().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-civilian",
   "metadata": {},
   "source": [
    "If we don't know _a priori_ what columns are dictionary-encoded (and might hold categoricals), we can find out by programmatically inspecting the Parquet metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dangerous-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Categorical', 'Color', 'YesNo'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_cols = set([])\n",
    "\n",
    "# get metadata for each partition\n",
    "for piece in pq.ParquetDataset(\"colors.parquet\", use_legacy_dataset=False).pieces:\n",
    "    meta = piece.metadata\n",
    "\n",
    "    # get column names\n",
    "    cols = enumerate(meta.schema.names)\n",
    "\n",
    "    # get column metadata for each row group\n",
    "    for i in range(meta.num_row_groups):\n",
    "        rg = meta.row_group(i)\n",
    "        for col, colname in cols:\n",
    "            if \"PLAIN_DICTIONARY\" in rg.column(col).encodings:\n",
    "                dictionary_cols.add(colname)\n",
    "\n",
    "dictionary_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-miracle",
   "metadata": {},
   "source": [
    "Preserving column types when transferring data from a JVM-based ETL pipeline to a Python-based machine learning pipeline can save a lot of human and computational effort -- and eliminate an entire class of performance regressions and bugs as well.  Fortunately, it just takes a little bit of care to ensure that our entire pipeline preserves the efficiency advantages of Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
