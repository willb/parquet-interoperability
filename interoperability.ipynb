{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interested-petersburg",
   "metadata": {},
   "source": [
    "# Parquet interoperability from the JVM to Python\n",
    "\n",
    "Apache Parquet is a great default choice for a data serialization format in data processing and machine learning pipelines, but just because it's available in many environments doesn't mean it has the same behavior everywhere. In the remaining discussion, we'll look at some potential interoperability headaches and show how to work around them.\n",
    "\n",
    "We'll start by looking at a Parquet file generated by Apache Spark with the output of an ETL job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-egyptian",
   "metadata": {},
   "source": [
    "We can look at the schema for this file and inspect a few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = session.read.parquet(\"berlin_payments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-durham",
   "metadata": {},
   "source": [
    "The \"file\" we're reading from (`berlin_payments.parquet`) is a partitioned Parquet file, so it's really a directory.  We can inspect the Parquet metadata for each column using the `parquet-tools` utility; this shows us that many of our columns are strings and that string columns are universally dictionary-encoded, which means that each string is stored as an index into a dictionary rather than as a literal value.  By storing values that may be repeated many times in this way, we save space and compute time.  (Parquet defaults to dictionary-encoding small-cardinality string columns, and we can assume that many of these will be treated as categoricals later in a data pipeline.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "!parquet-tools meta berlin_payments.parquet 2>& 1 | head -30 | grep SNAPPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-candle",
   "metadata": {},
   "source": [
    "So far, so good.  But what happens when we read these data into pandas?  We can load Parquet files into pandas if we have PyArrow installed; let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_parquet(\"berlin_payments.parquet\")\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-infection",
   "metadata": {},
   "source": [
    "The data look about like we'd expect them to.  However, when we look at how pandas is representing our data, we're in for a nasty surprise:  pandas has taken our efficiently dictionary-encoded strings and represented them with arbitrary Python objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-weather",
   "metadata": {},
   "source": [
    "We could convert each column to strings and then to categoricals, but this would be tedious and it is also certainly wasteful in terms of time and space.  (Note that if we'd created a pandas table with `string` or `categorical` `dtypes` and saved _that_ to Parquet, the types would survive a round-trip to disk because they'd be stored in pandas-specific Parquet metadata.)\n",
    "\n",
    "In this case, pandas is using the PyArrow Parquet backend; interestingly, if we use PyArrow directly and read into a `pyarrow.Table`, the string types are preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "arrow_table = pq.read_table(\"berlin_payments.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-glance",
   "metadata": {},
   "source": [
    "...but once we convert _that_ table to pandas, we've lost the type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_table.to_pandas().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-consultancy",
   "metadata": {},
   "source": [
    "However, we can force PyArrow to preserve the dictionary encoding even through the pandas conversion if we specify the `read_dictionary` option with a list of appropriate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_arrow_table = pq.read_table(\"berlin_payments.parquet/\", read_dictionary=['neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_arrow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_arrow_table.to_pandas().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-civilian",
   "metadata": {},
   "source": [
    "If we don't know _a priori_ what columns are dictionary-encoded (and might hold categoricals), we can find out by programmatically inspecting the Parquet metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_cols = set([])\n",
    "\n",
    "# get metadata for each partition\n",
    "for piece in pq.ParquetDataset(\"berlin_payments.parquet\", use_legacy_dataset=False).pieces:\n",
    "    meta = piece.metadata\n",
    "\n",
    "    # get column names\n",
    "    cols = enumerate(meta.schema.names)\n",
    "\n",
    "    # get column metadata for each row group\n",
    "    for i in range(meta.num_row_groups):\n",
    "        rg = meta.row_group(i)\n",
    "        for col, colname in cols:\n",
    "            if \"PLAIN_DICTIONARY\" in rg.column(col).encodings:\n",
    "                dictionary_cols.add(colname)\n",
    "\n",
    "dictionary_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-miracle",
   "metadata": {},
   "source": [
    "Preserving column types when transferring data from a JVM-based ETL pipeline to a Python-based machine learning pipeline can save a lot of human and computational effort -- and eliminate an entire class of performance regressions and bugs as well.  Fortunately, it just takes a little bit of care to ensure that our entire pipeline preserves the efficiency advantages of Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
